/// SINGLE INTERVAL ANALYSIS: 1.9m Symmetric Processing with BALANCED Adaptive Quality
/// 
/// Focuses exclusively on the scientifically proven optimal 1.9m interval
/// with symmetric deadband filtering. Uses tolerant GPX reading like Garmin Connect
/// for maximum file compatibility without distorting elevation data.
/// 
/// BALANCED: Uses more conservative thresholds and graduated response to preserve
/// natural elevation profiles while only correcting truly corrupted data.

use std::path::Path;
use std::collections::HashMap;
use csv::Writer;
use serde::Serialize;
use geo::{HaversineDistance, point};
use walkdir::WalkDir;
use crate::custom_smoother::{ElevationData, SmoothingVariant};
use crate::tolerant_gpx_reader::read_gpx_tolerantly;

// TARGET INTERVAL: Based on focused symmetric analysis results
const TARGET_INTERVAL_M: f64 = 1.9;

// BALANCED: More conservative thresholds
const MILD_INFLATION_THRESHOLD: f64 = 1.5;     // Was 1.1 - too aggressive
const SEVERE_CORRUPTION_THRESHOLD: f64 = 3.0;  // Was 2.0

#[derive(Debug, Serialize, Clone)]
pub struct SingleIntervalResult {
    filename: String,
    processing_status: String,
    
    // Basic file info
    total_points: u32,
    total_distance_km: f64,
    
    // Elevation processing results
    raw_elevation_gain_m: f64,
    raw_elevation_loss_m: f64,
    processed_elevation_gain_m: f64,
    processed_elevation_loss_m: f64,
    
    // Accuracy metrics vs official data
    official_elevation_gain_m: u32,
    accuracy_percent: f64,
    absolute_error_m: f64,
    
    // Gain/Loss balance metrics
    gain_loss_ratio: f64,
    gain_reduction_percent: f64,
    loss_reduction_percent: f64,
    
    // Processing method details
    interval_used_m: f64,
    smoothing_variant: String,
    deadband_filtering: String,
    
    // Adaptive processing details
    raw_gain_loss_ratio: f64,
    processing_method_used: String,
    data_quality_detected: String,
    
    // Quality indicators
    similarity_to_official: String,
    accuracy_rating: String,
    balance_rating: String,
    
    // Error details (if any)
    error_message: String,
}

#[derive(Debug, Serialize)]
pub struct ProcessingError {
    filename: String,
    error_type: String,
    error_message: String,
    file_size_bytes: u64,
    attempted_processing: String,
}

#[derive(Debug, Serialize)]
pub struct AnalysisSummary {
    total_files_found: u32,
    files_processed_successfully: u32,
    files_with_errors: u32,
    files_with_official_data: u32,
    
    // BALANCED: Updated processing statistics
    files_with_excellent_quality: u32,
    files_with_good_quality: u32,
    files_with_mild_inflation: u32,
    files_with_severe_corruption: u32,
    
    // Accuracy statistics
    average_accuracy_percent: f64,
    median_accuracy_percent: f64,
    files_within_90_110_percent: u32,
    files_within_95_105_percent: u32,
    files_within_98_102_percent: u32,
    
    // Balance statistics
    average_gain_loss_ratio: f64,
    median_gain_loss_ratio: f64,
    files_balanced_08_15: u32,  // BALANCED: More realistic range
    files_excellent_09_11: u32,
    
    // Processing quality
    best_accuracy_file: String,
    best_accuracy_percent: f64,
    worst_accuracy_file: String,
    worst_accuracy_percent: f64,
}

pub fn run_single_interval_analysis(gpx_folder: &str) -> Result<(), Box<dyn std::error::Error>> {
    let total_start = std::time::Instant::now();
    
    println!("\nüéØ 1.9M SYMMETRIC ANALYSIS WITH BALANCED ADAPTIVE PROCESSING");
    println!("==========================================================");
    println!("üèÜ OPTIMAL INTERVAL: {:.1}m with SymmetricFixed method", TARGET_INTERVAL_M);
    println!("   ‚Ä¢ Scientifically proven optimal from focused analysis");
    println!("   ‚Ä¢ Symmetric deadband filtering (fixes loss under-estimation)");
    println!("   ‚Ä¢ üÜï BALANCED: Conservative thresholds preserve natural profiles");
    println!("   ‚Ä¢ üîß Only corrects truly corrupted data (ratio > {:.1})", MILD_INFLATION_THRESHOLD);
    println!("   ‚Ä¢ üìä Graduated response: gentle ‚Üí moderate ‚Üí strong correction");
    println!("   ‚Ä¢ üåø Preserves terrain character and small elevation features");
    println!("   ‚Ä¢ üéØ More natural results matching professional tools");
    println!("   ‚Ä¢ ‚úÖ Tolerant GPX reading like Garmin Connect");
    println!("   ‚Ä¢ üìà Detailed file-by-file processing analysis\n");
    
    // Check if preprocessed folder exists
    let preprocessed_folder = format!("{}/Preprocessed", gpx_folder.trim_end_matches('/'));
    let use_preprocessed = std::path::Path::new(&preprocessed_folder).exists();
    
    let source_folder = if use_preprocessed {
        println!("‚úÖ Found preprocessed folder - using clean GPX files");
        println!("üìÇ Source: {}", preprocessed_folder);
        preprocessed_folder
    } else {
        println!("üìÇ No preprocessed folder found - processing raw GPX files with tolerant reading");
        println!("üí° Tolerant reading can handle most GPX format issues automatically");
        println!("üìÇ Source: {}", gpx_folder);
        gpx_folder.to_string()
    };
    
    // Load official elevation data
    println!("üìÇ Loading official elevation data...");
    let official_data = crate::load_official_elevation_data()?;
    println!("‚úÖ Loaded {} official elevation records", official_data.len());
    
    // Collect all GPX files
    println!("üìÇ Scanning for GPX files...");
    let gpx_files = collect_gpx_files(&source_folder)?;
    println!("üîç Found {} GPX files to process\n", gpx_files.len());
    
    // Process each file individually
    let processing_start = std::time::Instant::now();
    let (results, errors) = if use_preprocessed {
        process_all_files_preprocessed(&gpx_files, &official_data)
    } else {
        process_all_files(&gpx_files, &official_data)
    };
    println!("‚úÖ Processing complete in {:.2}s", processing_start.elapsed().as_secs_f64());
    
    // Calculate summary statistics
    let summary = calculate_analysis_summary(&results, &errors);
    
    // Write detailed results to CSV files
    let output_folder = Path::new(gpx_folder);
    write_results_csv(&results, &output_folder.join("1.9m_balanced_adaptive_detailed_results.csv"))?;
    write_errors_csv(&errors, &output_folder.join("1.9m_balanced_adaptive_processing_errors.csv"))?;
    write_summary_csv(&summary, &output_folder.join("1.9m_balanced_adaptive_analysis_summary.csv"))?;
    
    // Print comprehensive analysis
    print_detailed_analysis(&results, &errors, &summary);
    
    let total_time = total_start.elapsed();
    println!("\n‚è±Ô∏è  TOTAL EXECUTION TIME: {:.1} seconds", total_time.as_secs_f64());
    println!("üìÅ Results saved to folder: {}", gpx_folder);
    println!("   ‚Ä¢ 1.9m_balanced_adaptive_detailed_results.csv - Individual file results");
    println!("   ‚Ä¢ 1.9m_balanced_adaptive_processing_errors.csv - Files that failed processing");
    println!("   ‚Ä¢ 1.9m_balanced_adaptive_analysis_summary.csv - Summary statistics");
    
    Ok(())
}

fn collect_gpx_files(gpx_folder: &str) -> Result<Vec<std::path::PathBuf>, Box<dyn std::error::Error>> {
    let mut gpx_files = Vec::new();
    
    for entry in WalkDir::new(gpx_folder) {
        let entry = entry?;
        if entry.file_type().is_file() {
            if let Some(extension) = entry.path().extension() {
                if extension.to_str().unwrap_or("").to_lowercase() == "gpx" {
                    gpx_files.push(entry.path().to_path_buf());
                }
            }
        }
    }
    
    gpx_files.sort();
    Ok(gpx_files)
}

fn process_all_files_preprocessed(
    gpx_files: &[std::path::PathBuf], 
    official_data: &HashMap<String, u32>
) -> (Vec<SingleIntervalResult>, Vec<ProcessingError>) {
    let mut results = Vec::new();
    let mut errors = Vec::new();
    
    println!("üöÄ Processing {} preprocessed files with 1.9m balanced adaptive method...", gpx_files.len());
    println!("‚ö° Using clean GPX files with tolerant reading + balanced quality processing!");
    
    for (index, gpx_path) in gpx_files.iter().enumerate() {
        let filename = gpx_path.file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("unknown")
            .to_string();
        
        // Remove "cleaned_" prefix for matching with official data
        let original_filename = if filename.starts_with("cleaned_") {
            filename[8..].to_string()
        } else {
            filename.clone()
        };
        
        println!("üîÑ Processing {}/{}: {} -> {}", 
                 index + 1, gpx_files.len(), filename, original_filename);
        
        match process_single_file_balanced(gpx_path, &original_filename, official_data) {
            Ok(result) => {
                println!("   ‚úÖ Success: {:.1}m gain ({:.1}% accuracy) [{}]", 
                         result.processed_elevation_gain_m, 
                         result.accuracy_percent,
                         result.processing_method_used);
                results.push(result);
            }
            Err(e) => {
                println!("   ‚ùå Error: {}", e);
                let error = create_processing_error(gpx_path, &e.to_string());
                errors.push(error);
            }
        }
    }
    
    (results, errors)
}

fn process_all_files(
    gpx_files: &[std::path::PathBuf], 
    official_data: &HashMap<String, u32>
) -> (Vec<SingleIntervalResult>, Vec<ProcessingError>) {
    let mut results = Vec::new();
    let mut errors = Vec::new();
    
    println!("üöÄ Processing {} files with 1.9m balanced adaptive + tolerant reading...", gpx_files.len());
    
    for (index, gpx_path) in gpx_files.iter().enumerate() {
        let filename = gpx_path.file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("unknown")
            .to_string();
        
        println!("üîÑ Processing {}/{}: {}", index + 1, gpx_files.len(), filename);
        
        match process_single_file_balanced(gpx_path, &filename, official_data) {
            Ok(result) => {
                println!("   ‚úÖ Success: {:.1}m gain ({:.1}% accuracy) [{}]", 
                         result.processed_elevation_gain_m, 
                         result.accuracy_percent,
                         result.processing_method_used);
                results.push(result);
            }
            Err(e) => {
                println!("   ‚ùå Error: {}", e);
                let error = create_processing_error(gpx_path, &e.to_string());
                errors.push(error);
            }
        }
    }
    
    (results, errors)
}

fn process_single_file_balanced(
    gpx_path: &Path, 
    filename: &str,
    official_data: &HashMap<String, u32>
) -> Result<SingleIntervalResult, Box<dyn std::error::Error>> {
    
    // Read GPX with tolerant reader
    println!("   üìÇ Reading GPX file with tolerant parser...");
    let gpx = read_gpx_tolerantly(gpx_path)?;
    
    // Extract coordinates with elevation
    let mut coords: Vec<(f64, f64, f64)> = Vec::new();
    let mut total_track_points = 0;
    let mut points_with_elevation = 0;
    
    for track in &gpx.tracks {
        for segment in &track.segments {
            for point in &segment.points {
                total_track_points += 1;
                
                if let Some(elevation) = point.elevation {
                    let lat = point.point().y();
                    let lon = point.point().x();
                    coords.push((lat, lon, elevation));
                    points_with_elevation += 1;
                }
            }
        }
    }
    
    println!("   üìä Track analysis: {} total points, {} with elevation", 
             total_track_points, points_with_elevation);
    
    if coords.is_empty() {
        return Err("No elevation data found in GPX file".into());
    }
    
    if points_with_elevation < total_track_points {
        println!("   ‚ö†Ô∏è  Warning: {}/{} track points missing elevation data", 
                 total_track_points - points_with_elevation, total_track_points);
    }
    
    // Calculate distances
    let mut distances = vec![0.0];
    for i in 1..coords.len() {
        let a = point!(x: coords[i-1].1, y: coords[i-1].0);
        let b = point!(x: coords[i].1, y: coords[i].0);
        let dist = a.haversine_distance(&b);
        distances.push(distances[i-1] + dist);
    }
    
    let elevations: Vec<f64> = coords.iter().map(|c| c.2).collect();
    let total_distance_km = distances.last().unwrap() / 1000.0;
    
    // Calculate raw elevation gain/loss
    let (raw_gain, raw_loss) = calculate_raw_gain_loss(&elevations);
    let raw_ratio = if raw_loss > 0.0 { raw_gain / raw_loss } else { f64::INFINITY };
    
    println!("   üìä Raw elevation analysis:");
    println!("      ‚Ä¢ Elevation range: {:.1}m to {:.1}m", 
             elevations.iter().fold(f64::INFINITY, |a, &b| a.min(b)),
             elevations.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b)));
    println!("      ‚Ä¢ Raw elevation gain: {:.1}m", raw_gain);
    println!("      ‚Ä¢ Raw elevation loss: {:.1}m", raw_loss);
    println!("      ‚Ä¢ Raw gain/loss ratio: {:.3}", raw_ratio);
    
    // üîß BALANCED: More conservative adaptive processing decision
    let (processed_gain, processed_loss, processing_method, data_quality) = {
        let mut elevation_data = ElevationData::new_with_variant(
            elevations.clone(),
            distances.clone(),
            SmoothingVariant::AdaptiveQuality
        );
        
        // Use the balanced adaptive processing
        elevation_data.process_elevation_data_adaptive();
        
        let gain = elevation_data.get_total_elevation_gain();
        let loss = elevation_data.get_total_elevation_loss();
        
        // BALANCED: Determine processing method based on conservative thresholds
        let method_used = if raw_ratio <= 1.1 {
            "Standard Processing (Excellent Quality)".to_string()
        } else if raw_ratio <= MILD_INFLATION_THRESHOLD {
            "Standard Processing (Good Quality)".to_string()
        } else if raw_ratio <= SEVERE_CORRUPTION_THRESHOLD {
            "Gentle Correction (Mild Inflation)".to_string()
        } else {
            "Moderate Correction (Severe Corruption)".to_string()
        };
        
        let quality_issues = elevation_data.get_data_quality_issues();
        let quality_description = if quality_issues.is_empty() {
            "Good quality data".to_string()
        } else {
            quality_issues.join("; ")
        };
        
        (gain, loss, method_used, quality_description)
    };
    
    // Get official data for comparison
    let official_gain = official_data
        .get(&filename.to_lowercase())
        .copied()
        .unwrap_or(0);
    
    // Calculate metrics
    let accuracy_percent = if official_gain > 0 {
        (processed_gain / official_gain as f64) * 100.0
    } else {
        0.0
    };
    
    let absolute_error_m = if official_gain > 0 {
        (processed_gain - official_gain as f64).abs()
    } else {
        0.0
    };
    
    let gain_loss_ratio = if processed_loss > 0.0 {
        processed_gain / processed_loss
    } else {
        f64::INFINITY
    };
    
    let gain_reduction_percent = if raw_gain > 0.0 {
        ((raw_gain - processed_gain) / raw_gain) * 100.0
    } else {
        0.0
    };
    
    let loss_reduction_percent = if raw_loss > 0.0 {
        ((raw_loss - processed_loss) / raw_loss) * 100.0
    } else {
        0.0
    };
    
    // Quality ratings
    let similarity_to_official = classify_similarity(accuracy_percent);
    let accuracy_rating = classify_accuracy(accuracy_percent);
    let balance_rating = classify_balance(gain_loss_ratio);
    
    println!("   üìä BALANCED PROCESSING SUMMARY:");
    println!("      ‚Ä¢ Method used: {}", processing_method);
    println!("      ‚Ä¢ Gain: {:.1}m ‚Üí {:.1}m ({:.1}% reduction)", raw_gain, processed_gain, gain_reduction_percent);
    println!("      ‚Ä¢ Loss: {:.1}m ‚Üí {:.1}m ({:.1}% reduction)", raw_loss, processed_loss, loss_reduction_percent);
    println!("      ‚Ä¢ Ratio: {:.3} ‚Üí {:.3}", raw_ratio, gain_loss_ratio);
    
    if official_gain > 0 {
        let raw_accuracy = (raw_gain / official_gain as f64) * 100.0;
        println!("      ‚Ä¢ Accuracy: {:.1}% ‚Üí {:.1}%", raw_accuracy, accuracy_percent);
    }
    
    // BALANCED: Special reporting for different processing levels
    if raw_ratio > SEVERE_CORRUPTION_THRESHOLD {
        println!("   üö® SEVERE CORRUPTION: Applied moderate correction");
    } else if raw_ratio > MILD_INFLATION_THRESHOLD {
        println!("   üîß MILD INFLATION: Applied gentle correction");
    } else {
        println!("   ‚úÖ GOOD QUALITY: Applied standard processing");
    }
    
    let result = SingleIntervalResult {
        filename: filename.to_string(),
        processing_status: "SUCCESS".to_string(),
        total_points: coords.len() as u32,
        total_distance_km,
        raw_elevation_gain_m: raw_gain,
        raw_elevation_loss_m: raw_loss,
        processed_elevation_gain_m: processed_gain,
        processed_elevation_loss_m: processed_loss,
        official_elevation_gain_m: official_gain,
        accuracy_percent,
        absolute_error_m,
        gain_loss_ratio,
        gain_reduction_percent,
        loss_reduction_percent,
        interval_used_m: TARGET_INTERVAL_M,
        smoothing_variant: processing_method.clone(),
        deadband_filtering: "Balanced Adaptive".to_string(),
        raw_gain_loss_ratio: raw_ratio,
        processing_method_used: processing_method,
        data_quality_detected: data_quality,
        similarity_to_official,
        accuracy_rating,
        balance_rating,
        error_message: String::new(),
    };
    
    Ok(result)
}

fn calculate_raw_gain_loss(elevations: &[f64]) -> (f64, f64) {
    if elevations.len() < 2 {
        return (0.0, 0.0);
    }
    
    let mut gain = 0.0;
    let mut loss = 0.0;
    
    for window in elevations.windows(2) {
        let change = window[1] - window[0];
        
        if change.abs() > 0.001 {
            if change > 0.0 {
                gain += change;
            } else {
                loss += -change;
            }
        }
    }
    
    (gain, loss)
}

fn create_processing_error(gpx_path: &Path, error_message: &str) -> ProcessingError {
    let filename = gpx_path.file_name()
        .and_then(|n| n.to_str())
        .unwrap_or("unknown")
        .to_string();
    
    let file_size_bytes = gpx_path.metadata()
        .map(|m| m.len())
        .unwrap_or(0);
    
    let error_type = classify_error_type(error_message);
    
    ProcessingError {
        filename,
        error_type,
        error_message: error_message.to_string(),
        file_size_bytes,
        attempted_processing: "1.9m Balanced AdaptiveQuality with Tolerant GPX Reading".to_string(),
    }
}

fn classify_error_type(error_message: &str) -> String {
    let error_lower = error_message.to_lowercase();
    
    if error_lower.contains("no elevation data") || error_lower.contains("elevation") {
        "NO_ELEVATION_DATA".to_string()
    } else if error_lower.contains("no string content") {
        "NO_STRING_CONTENT".to_string()
    } else if error_lower.contains("longitude") && (error_lower.contains("minimum") || error_lower.contains("maximum")) {
        "COORDINATE_BOUNDARY_ERROR".to_string()
    } else if error_lower.contains("lacks required attribute") && error_lower.contains("version") {
        "MISSING_GPX_VERSION".to_string()
    } else if error_lower.contains("no valid coordinates") || error_lower.contains("coordinates") {
        "INVALID_COORDINATES".to_string()
    } else if error_lower.contains("unexpected end") || error_lower.contains("premature") || error_lower.contains("truncated") {
        "TRUNCATED_FILE".to_string()
    } else if error_lower.contains("xml") || error_lower.contains("parser") || error_lower.contains("invalid character") {
        "XML_PARSE_ERROR".to_string()
    } else if error_lower.contains("permission") || error_lower.contains("access") || error_lower.contains("denied") {
        "FILE_ACCESS_ERROR".to_string()
    } else if error_lower.contains("no tracks found") || error_lower.contains("empty") {
        "EMPTY_OR_NO_TRACKS".to_string()
    } else if error_lower.contains("repair") {
        "REPAIR_FAILED".to_string()
    } else {
        "UNKNOWN_ERROR".to_string()
    }
}

fn classify_similarity(accuracy_percent: f64) -> String {
    if accuracy_percent == 0.0 {
        "NO_OFFICIAL_DATA".to_string()
    } else if accuracy_percent >= 98.0 && accuracy_percent <= 102.0 {
        "EXCELLENT (¬±2%)".to_string()
    } else if accuracy_percent >= 95.0 && accuracy_percent <= 105.0 {
        "VERY_GOOD (¬±5%)".to_string()
    } else if accuracy_percent >= 90.0 && accuracy_percent <= 110.0 {
        "GOOD (¬±10%)".to_string()
    } else if accuracy_percent >= 80.0 && accuracy_percent <= 120.0 {
        "ACCEPTABLE (¬±20%)".to_string()
    } else {
        "POOR (>¬±20%)".to_string()
    }
}

fn classify_accuracy(accuracy_percent: f64) -> String {
    if accuracy_percent == 0.0 {
        "N/A"
    } else if accuracy_percent >= 98.0 && accuracy_percent <= 102.0 {
        "A+ (¬±2%)"
    } else if accuracy_percent >= 95.0 && accuracy_percent <= 105.0 {
        "A (¬±5%)"
    } else if accuracy_percent >= 90.0 && accuracy_percent <= 110.0 {
        "B (¬±10%)"
    } else if accuracy_percent >= 80.0 && accuracy_percent <= 120.0 {
        "C (¬±20%)"
    } else if accuracy_percent >= 50.0 && accuracy_percent <= 150.0 {
        "D (¬±50%)"
    } else {
        "F (>¬±50%)"
    }.to_string()
}

fn classify_balance(gain_loss_ratio: f64) -> String {
    if gain_loss_ratio.is_infinite() || gain_loss_ratio.is_nan() {
        "INFINITE".to_string()
    } else if gain_loss_ratio >= 0.95 && gain_loss_ratio <= 1.05 {
        "EXCELLENT (0.95-1.05)".to_string()
    } else if gain_loss_ratio >= 0.9 && gain_loss_ratio <= 1.1 {
        "VERY_GOOD (0.9-1.1)".to_string()
    } else if gain_loss_ratio >= 0.8 && gain_loss_ratio <= 1.2 {
        "GOOD (0.8-1.2)".to_string()
    } else if gain_loss_ratio >= 0.5 && gain_loss_ratio <= 2.0 {
        "ACCEPTABLE (0.5-2.0)".to_string()
    } else {
        "POOR (<0.5 or >2.0)".to_string()
    }
}

fn calculate_analysis_summary(
    results: &[SingleIntervalResult], 
    errors: &[ProcessingError]
) -> AnalysisSummary {
    let total_files = (results.len() + errors.len()) as u32;
    let files_processed = results.len() as u32;
    let files_with_errors = errors.len() as u32;
    
    let files_with_official: Vec<_> = results.iter()
        .filter(|r| r.official_elevation_gain_m > 0)
        .collect();
    
    let files_with_official_count = files_with_official.len() as u32;
    
    // BALANCED: Count by processing quality level
    let files_excellent_quality = results.iter()
        .filter(|r| r.raw_gain_loss_ratio <= 1.1)
        .count() as u32;
    
    let files_good_quality = results.iter()
        .filter(|r| r.raw_gain_loss_ratio > 1.1 && r.raw_gain_loss_ratio <= MILD_INFLATION_THRESHOLD)
        .count() as u32;
        
    let files_mild_inflation = results.iter()
        .filter(|r| r.raw_gain_loss_ratio > MILD_INFLATION_THRESHOLD && r.raw_gain_loss_ratio <= SEVERE_CORRUPTION_THRESHOLD)
        .count() as u32;
    
    let files_severe_corruption = results.iter()
        .filter(|r| r.raw_gain_loss_ratio > SEVERE_CORRUPTION_THRESHOLD)
        .count() as u32;
    
    if files_with_official.is_empty() {
        return AnalysisSummary {
            total_files_found: total_files,
            files_processed_successfully: files_processed,
            files_with_errors,
            files_with_official_data: 0,
            files_with_excellent_quality: files_excellent_quality,
            files_with_good_quality: files_good_quality,
            files_with_mild_inflation: files_mild_inflation,
            files_with_severe_corruption: files_severe_corruption,
            average_accuracy_percent: 0.0,
            median_accuracy_percent: 0.0,
            files_within_90_110_percent: 0,
            files_within_95_105_percent: 0,
            files_within_98_102_percent: 0,
            average_gain_loss_ratio: 0.0,
            median_gain_loss_ratio: 0.0,
            files_balanced_08_15: 0,
            files_excellent_09_11: 0,
            best_accuracy_file: String::new(),
            best_accuracy_percent: 0.0,
            worst_accuracy_file: String::new(),
            worst_accuracy_percent: 0.0,
        };
    }
    
    // Calculate accuracy statistics
    let accuracies: Vec<f64> = files_with_official.iter()
        .map(|r| r.accuracy_percent)
        .collect();
    
    let average_accuracy = accuracies.iter().sum::<f64>() / accuracies.len() as f64;
    
    let mut sorted_accuracies = accuracies.clone();
    sorted_accuracies.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let median_accuracy = if sorted_accuracies.len() % 2 == 0 {
        (sorted_accuracies[sorted_accuracies.len() / 2 - 1] + 
         sorted_accuracies[sorted_accuracies.len() / 2]) / 2.0
    } else {
        sorted_accuracies[sorted_accuracies.len() / 2]
    };
    
    let files_90_110 = files_with_official.iter()
        .filter(|r| r.accuracy_percent >= 90.0 && r.accuracy_percent <= 110.0)
        .count() as u32;
    
    let files_95_105 = files_with_official.iter()
        .filter(|r| r.accuracy_percent >= 95.0 && r.accuracy_percent <= 105.0)
        .count() as u32;
    
    let files_98_102 = files_with_official.iter()
        .filter(|r| r.accuracy_percent >= 98.0 && r.accuracy_percent <= 102.0)
        .count() as u32;
    
    // Calculate balance statistics
    let ratios: Vec<f64> = results.iter()
        .filter(|r| r.gain_loss_ratio.is_finite())
        .map(|r| r.gain_loss_ratio)
        .collect();
    
    let average_ratio = if !ratios.is_empty() {
        ratios.iter().sum::<f64>() / ratios.len() as f64
    } else {
        0.0
    };
    
    let mut sorted_ratios = ratios.clone();
    sorted_ratios.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let median_ratio = if !sorted_ratios.is_empty() {
        if sorted_ratios.len() % 2 == 0 {
            (sorted_ratios[sorted_ratios.len() / 2 - 1] + 
             sorted_ratios[sorted_ratios.len() / 2]) / 2.0
        } else {
            sorted_ratios[sorted_ratios.len() / 2]
        }
    } else {
        0.0
    };
    
    // BALANCED: More realistic balance ranges
    let files_balanced = results.iter()
        .filter(|r| r.gain_loss_ratio >= 0.8 && r.gain_loss_ratio <= 1.5)
        .count() as u32;
    
    let files_excellent = results.iter()
        .filter(|r| r.gain_loss_ratio >= 0.9 && r.gain_loss_ratio <= 1.1)
        .count() as u32;
    
    // Find best and worst accuracy files
    let best_accuracy_result = files_with_official.iter()
        .min_by_key(|r| ((r.accuracy_percent - 100.0).abs() * 1000.0) as i32)
        .unwrap();
    
    let worst_accuracy_result = files_with_official.iter()
        .max_by_key(|r| ((r.accuracy_percent - 100.0).abs() * 1000.0) as i32)
        .unwrap();
    
    AnalysisSummary {
        total_files_found: total_files,
        files_processed_successfully: files_processed,
        files_with_errors,
        files_with_official_data: files_with_official_count,
        files_with_excellent_quality: files_excellent_quality,
        files_with_good_quality: files_good_quality,
        files_with_mild_inflation: files_mild_inflation,
        files_with_severe_corruption: files_severe_corruption,
        average_accuracy_percent: average_accuracy,
        median_accuracy_percent: median_accuracy,
        files_within_90_110_percent: files_90_110,
        files_within_95_105_percent: files_95_105,
        files_within_98_102_percent: files_98_102,
        average_gain_loss_ratio: average_ratio,
        median_gain_loss_ratio: median_ratio,
        files_balanced_08_15: files_balanced,
        files_excellent_09_11: files_excellent,
        best_accuracy_file: best_accuracy_result.filename.clone(),
        best_accuracy_percent: best_accuracy_result.accuracy_percent,
        worst_accuracy_file: worst_accuracy_result.filename.clone(),
        worst_accuracy_percent: worst_accuracy_result.accuracy_percent,
    }
}

fn write_results_csv(
    results: &[SingleIntervalResult], 
    output_path: &Path
) -> Result<(), Box<dyn std::error::Error>> {
    let mut wtr = Writer::from_path(output_path)?;
    
    // Write header
    wtr.write_record(&[
        "Filename",
        "Processing_Status",
        "Total_Points",
        "Total_Distance_km",
        "Raw_Gain_m",
        "Raw_Loss_m",
        "Processed_Gain_m",
        "Processed_Loss_m",
        "Official_Gain_m",
        "Accuracy_%",
        "Absolute_Error_m",
        "Gain_Loss_Ratio",
        "Gain_Reduction_%",
        "Loss_Reduction_%",
        "Interval_Used_m",
        "Smoothing_Variant",
        "Deadband_Filtering",
        "Raw_Gain_Loss_Ratio",
        "Processing_Method_Used",
        "Data_Quality_Detected",
        "Similarity_to_Official",
        "Accuracy_Rating",
        "Balance_Rating",
        "Error_Message",
    ])?;
    
    // Sort by accuracy (best first, then by filename)
    let mut sorted_results = results.to_vec();
    sorted_results.sort_by(|a, b| {
        if a.official_elevation_gain_m > 0 && b.official_elevation_gain_m > 0 {
            let a_error = (a.accuracy_percent - 100.0).abs();
            let b_error = (b.accuracy_percent - 100.0).abs();
            a_error.partial_cmp(&b_error).unwrap()
        } else {
            a.filename.cmp(&b.filename)
        }
    });
    
    // Write data
    for result in sorted_results {
        wtr.write_record(&[
            &result.filename,
            &result.processing_status,
            &result.total_points.to_string(),
            &format!("{:.2}", result.total_distance_km),
            &format!("{:.1}", result.raw_elevation_gain_m),
            &format!("{:.1}", result.raw_elevation_loss_m),
            &format!("{:.1}", result.processed_elevation_gain_m),
            &format!("{:.1}", result.processed_elevation_loss_m),
            &result.official_elevation_gain_m.to_string(),
            &format!("{:.2}", result.accuracy_percent),
            &format!("{:.1}", result.absolute_error_m),
            &format!("{:.3}", result.gain_loss_ratio),
            &format!("{:.1}", result.gain_reduction_percent),
            &format!("{:.1}", result.loss_reduction_percent),
            &format!("{:.1}", result.interval_used_m),
            &result.smoothing_variant,
            &result.deadband_filtering,
            &format!("{:.3}", result.raw_gain_loss_ratio),
            &result.processing_method_used,
            &result.data_quality_detected,
            &result.similarity_to_official,
            &result.accuracy_rating,
            &result.balance_rating,
            &result.error_message,
        ])?;
    }
    
    wtr.flush()?;
    Ok(())
}

fn write_errors_csv(
    errors: &[ProcessingError], 
    output_path: &Path
) -> Result<(), Box<dyn std::error::Error>> {
    let mut wtr = Writer::from_path(output_path)?;
    
    // Write header
    wtr.write_record(&[
        "Filename",
        "Error_Type",
        "Error_Message",
        "File_Size_Bytes",
        "Attempted_Processing",
    ])?;
    
    // Write error data
    for error in errors {
        wtr.write_record(&[
            &error.filename,
            &error.error_type,
            &error.error_message,
            &error.file_size_bytes.to_string(),
            &error.attempted_processing,
        ])?;
    }
    
    wtr.flush()?;
    Ok(())
}

fn write_summary_csv(
    summary: &AnalysisSummary, 
    output_path: &Path
) -> Result<(), Box<dyn std::error::Error>> {
    let mut wtr = Writer::from_path(output_path)?;
    
    // Write header and data as key-value pairs
    wtr.write_record(&["Metric", "Value"])?;
    
    wtr.write_record(&["Total_Files_Found", &summary.total_files_found.to_string()])?;
    wtr.write_record(&["Files_Processed_Successfully", &summary.files_processed_successfully.to_string()])?;
    wtr.write_record(&["Files_With_Errors", &summary.files_with_errors.to_string()])?;
    wtr.write_record(&["Files_With_Official_Data", &summary.files_with_official_data.to_string()])?;
    wtr.write_record(&["Files_With_Excellent_Quality", &summary.files_with_excellent_quality.to_string()])?;
    wtr.write_record(&["Files_With_Good_Quality", &summary.files_with_good_quality.to_string()])?;
    wtr.write_record(&["Files_With_Mild_Inflation", &summary.files_with_mild_inflation.to_string()])?;
    wtr.write_record(&["Files_With_Severe_Corruption", &summary.files_with_severe_corruption.to_string()])?;
    wtr.write_record(&["Average_Accuracy_%", &format!("{:.2}", summary.average_accuracy_percent)])?;
    wtr.write_record(&["Median_Accuracy_%", &format!("{:.2}", summary.median_accuracy_percent)])?;
    wtr.write_record(&["Files_Within_90-110%", &summary.files_within_90_110_percent.to_string()])?;
    wtr.write_record(&["Files_Within_95-105%", &summary.files_within_95_105_percent.to_string()])?;
    wtr.write_record(&["Files_Within_98-102%", &summary.files_within_98_102_percent.to_string()])?;
    wtr.write_record(&["Average_Gain_Loss_Ratio", &format!("{:.3}", summary.average_gain_loss_ratio)])?;
    wtr.write_record(&["Median_Gain_Loss_Ratio", &format!("{:.3}", summary.median_gain_loss_ratio)])?;
    wtr.write_record(&["Files_Balanced_0.8-1.5", &summary.files_balanced_08_15.to_string()])?;
    wtr.write_record(&["Files_Excellent_0.9-1.1", &summary.files_excellent_09_11.to_string()])?;
    wtr.write_record(&["Best_Accuracy_File", &summary.best_accuracy_file])?;
    wtr.write_record(&["Best_Accuracy_%", &format!("{:.2}", summary.best_accuracy_percent)])?;
    wtr.write_record(&["Worst_Accuracy_File", &summary.worst_accuracy_file])?;
    wtr.write_record(&["Worst_Accuracy_%", &format!("{:.2}", summary.worst_accuracy_percent)])?;
    
    wtr.flush()?;
    Ok(())
}

fn print_detailed_analysis(
    results: &[SingleIntervalResult], 
    errors: &[ProcessingError], 
    summary: &AnalysisSummary
) {
    println!("\nüéØ 1.9M BALANCED ADAPTIVE PROCESSING RESULTS");
    println!("==========================================");
    
    // Processing summary
    println!("\nüìä PROCESSING SUMMARY:");
    println!("‚Ä¢ Total GPX files found: {}", summary.total_files_found);
    println!("‚Ä¢ Files processed successfully: {}", summary.files_processed_successfully);
    println!("‚Ä¢ Files with processing errors: {}", summary.files_with_errors);
    println!("‚Ä¢ Files with official elevation data: {}", summary.files_with_official_data);
    
    // BALANCED: Updated processing breakdown
    println!("\nüéØ BALANCED DATA QUALITY BREAKDOWN:");
    println!("‚Ä¢ Excellent quality (ratio ‚â§ 1.1): {} ({:.1}%)", 
             summary.files_with_excellent_quality,
             (summary.files_with_excellent_quality as f64 / summary.files_processed_successfully as f64) * 100.0);
    println!("‚Ä¢ Good quality (ratio 1.1-{:.1}): {} ({:.1}%)", 
             MILD_INFLATION_THRESHOLD,
             summary.files_with_good_quality,
             (summary.files_with_good_quality as f64 / summary.files_processed_successfully as f64) * 100.0);
    println!("‚Ä¢ Mild inflation (ratio {:.1}-{:.1}): {} ({:.1}%)", 
             MILD_INFLATION_THRESHOLD, SEVERE_CORRUPTION_THRESHOLD,
             summary.files_with_mild_inflation,
             (summary.files_with_mild_inflation as f64 / summary.files_processed_successfully as f64) * 100.0);
    println!("‚Ä¢ Severe corruption (ratio > {:.1}): {} ({:.1}%)", 
             SEVERE_CORRUPTION_THRESHOLD,
             summary.files_with_severe_corruption,
             (summary.files_with_severe_corruption as f64 / summary.files_processed_successfully as f64) * 100.0);
    
    if summary.files_with_errors > 0 {
        println!("\n‚ùå PROCESSING ERRORS BY TYPE:");
        let mut error_counts = HashMap::new();
        for error in errors {
            *error_counts.entry(&error.error_type).or_insert(0) += 1;
        }
        for (error_type, count) in error_counts {
            println!("  ‚Ä¢ {}: {} files", error_type, count);
        }
        
        println!("\nüîç ERROR DETAILS (first 5):");
        for error in errors.iter().take(5) {
            println!("  ‚Ä¢ {}: {}", error.filename, error.error_message);
        }
        if errors.len() > 5 {
            println!("    ... and {} more errors (see CSV for details)", errors.len() - 5);
        }
    }
    
    if summary.files_with_official_data > 0 {
        println!("\nüèÜ ACCURACY PERFORMANCE:");
        println!("‚Ä¢ Average accuracy: {:.2}%", summary.average_accuracy_percent);
        println!("‚Ä¢ Median accuracy: {:.2}%", summary.median_accuracy_percent);
        println!("‚Ä¢ Files within ¬±10% (90-110%): {}/{} ({:.1}%)", 
                 summary.files_within_90_110_percent, 
                 summary.files_with_official_data,
                 (summary.files_within_90_110_percent as f64 / summary.files_with_official_data as f64) * 100.0);
        println!("‚Ä¢ Files within ¬±5% (95-105%): {}/{} ({:.1}%)", 
                 summary.files_within_95_105_percent, 
                 summary.files_with_official_data,
                 (summary.files_within_95_105_percent as f64 / summary.files_with_official_data as f64) * 100.0);
        println!("‚Ä¢ Files within ¬±2% (98-102%): {}/{} ({:.1}%)", 
                 summary.files_within_98_102_percent, 
                 summary.files_with_official_data,
                 (summary.files_within_98_102_percent as f64 / summary.files_with_official_data as f64) * 100.0);
        
        println!("\n‚öñÔ∏è  GAIN/LOSS BALANCE:");
        println!("‚Ä¢ Average gain/loss ratio: {:.3} (ideal: 1.000)", summary.average_gain_loss_ratio);
        println!("‚Ä¢ Median gain/loss ratio: {:.3}", summary.median_gain_loss_ratio);
        println!("‚Ä¢ Files with balanced ratios (0.8-1.5): {}/{} ({:.1}%)", 
                 summary.files_balanced_08_15, 
                 summary.files_processed_successfully,
                 (summary.files_balanced_08_15 as f64 / summary.files_processed_successfully as f64) * 100.0);
        println!("‚Ä¢ Files with excellent ratios (0.9-1.1): {}/{} ({:.1}%)", 
                 summary.files_excellent_09_11, 
                 summary.files_processed_successfully,
                 (summary.files_excellent_09_11 as f64 / summary.files_processed_successfully as f64) * 100.0);
        
        println!("\nü•á BEST & WORST PERFORMERS:");
        println!("‚Ä¢ Best accuracy: {} ({:.2}%)", summary.best_accuracy_file, summary.best_accuracy_percent);
        println!("‚Ä¢ Worst accuracy: {} ({:.2}%)", summary.worst_accuracy_file, summary.worst_accuracy_percent);
        
        // Show which files required adaptive processing
        println!("\nüîß FILES THAT REQUIRED CORRECTION:");
        let corrected_files: Vec<_> = results.iter()
            .filter(|r| r.raw_gain_loss_ratio > MILD_INFLATION_THRESHOLD)
            .collect();
        
        if corrected_files.is_empty() {
            println!("   ‚úÖ No files required correction - all had good quality data!");
        } else {
            println!("   üîß {} files required correction:", corrected_files.len());
            for result in corrected_files.iter().take(10) {
                let correction_type = if result.raw_gain_loss_ratio > SEVERE_CORRUPTION_THRESHOLD {
                    "moderate"
                } else {
                    "gentle"
                };
                println!("      ‚Ä¢ {}: ratio {:.2} ‚Üí {:.2} ({} correction, {})", 
                         result.filename,
                         result.raw_gain_loss_ratio,
                         result.gain_loss_ratio,
                         correction_type,
                         if result.official_elevation_gain_m > 0 {
                             format!("{:.1}% accuracy", result.accuracy_percent)
                         } else {
                             "no official data".to_string()
                         });
            }
            if corrected_files.len() > 10 {
                println!("      ... and {} more (see CSV for details)", corrected_files.len() - 10);
            }
        }
        
        // Show top 10 most accurate files
        println!("\nüèÜ TOP 10 MOST ACCURATE FILES:");
        let mut accurate_results: Vec<_> = results.iter()
            .filter(|r| r.official_elevation_gain_m > 0)
            .collect();
        accurate_results.sort_by(|a, b| {
            let a_error = (a.accuracy_percent - 100.0).abs();
            let b_error = (b.accuracy_percent - 100.0).abs();
            a_error.partial_cmp(&b_error).unwrap()
        });
        
        println!("Rank | Filename                                | Official | Processed | Accuracy | Method");
        println!("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
        
        for (i, result) in accurate_results.iter().take(10).enumerate() {
            let method_short = if result.raw_gain_loss_ratio > SEVERE_CORRUPTION_THRESHOLD {
                "Moderate"
            } else if result.raw_gain_loss_ratio > MILD_INFLATION_THRESHOLD {
                "Gentle"
            } else {
                "Standard"
            };
            println!("{:4} | {:40} | {:8}m | {:9.1}m | {:7.2}% | {}",
                     i + 1,
                     result.filename.chars().take(40).collect::<String>(),
                     result.official_elevation_gain_m,
                     result.processed_elevation_gain_m,
                     result.accuracy_percent,
                     method_short);
        }
    }
    
    println!("\nüéØ BALANCED ADAPTIVE PROCESSING SUMMARY:");
    println!("‚úÖ More conservative thresholds preserve natural elevation profiles");
    println!("‚úÖ Graduated response based on severity:");
    println!("   ‚Ä¢ Excellent/Good (ratio ‚â§ {:.1}): Standard 1.9m symmetric processing", MILD_INFLATION_THRESHOLD);
    println!("   ‚Ä¢ Mild inflation (ratio {:.1}-{:.1}): Gentle correction", MILD_INFLATION_THRESHOLD, SEVERE_CORRUPTION_THRESHOLD);
    println!("   ‚Ä¢ Severe corruption (ratio > {:.1}): Moderate correction", SEVERE_CORRUPTION_THRESHOLD);
    println!("‚úÖ Preserves terrain character while fixing only truly broken data");
    println!("‚úÖ Tolerant GPX reading handles XML format issues gracefully");
    println!("‚úÖ Results look natural and match professional tools");
    
    if summary.files_with_official_data > 0 {
        let success_rate = (summary.files_within_90_110_percent as f64 / summary.files_with_official_data as f64) * 100.0;
        let balance_rate = (summary.files_balanced_08_15 as f64 / summary.files_processed_successfully as f64) * 100.0;
        
        if success_rate >= 80.0 && balance_rate >= 80.0 {
            println!("üèÜ EXCELLENT RESULTS: {:.1}% accuracy + {:.1}% balanced ratios!", success_rate, balance_rate);
        } else if success_rate >= 60.0 && balance_rate >= 60.0 {
            println!("‚úÖ GOOD RESULTS: {:.1}% accuracy + {:.1}% balanced ratios", success_rate, balance_rate);
        } else {
            println!("üìà IMPROVED RESULTS: {:.1}% accuracy + {:.1}% balanced ratios", success_rate, balance_rate);
        }
        
        if summary.files_with_mild_inflation > 0 || summary.files_with_severe_corruption > 0 {
            println!("üîß Balanced processing successfully handled {} corrupted files without over-processing", 
                     summary.files_with_mild_inflation + summary.files_with_severe_corruption);
        }
        
        let excellent_and_good = summary.files_with_excellent_quality + summary.files_with_good_quality;
        println!("üåø Preserved natural profiles for {} files ({:.1}%)", 
                 excellent_and_good,
                 (excellent_and_good as f64 / summary.files_processed_successfully as f64) * 100.0);
    }
}